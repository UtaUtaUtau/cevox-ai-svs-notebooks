{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMDihqKvhLiNvkNv1xXI00Q"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# CeVOX DiffSinger Training Notebook\n","\n","![Ah shit, here we go again.](https://cdn.discordapp.com/attachments/455377490399985676/1141634804337606676/hq720.png)"],"metadata":{"id":"RhIT4Uf0ZST5"}},{"cell_type":"markdown","source":["# Setup Drive and stuff"],"metadata":{"id":"QEqYMcJ6aLGb"}},{"cell_type":"code","source":["#@title Install DiffSinger\n","\n","from IPython.display import clear_output\n","\n","clear_output()\n","print('Cloning DiffSinger')\n","!git clone https://github.com/openvpi/DiffSinger\n","%cd DiffSinger\n","\n","clear_output()\n","print('Installing requirements')\n","%pip install -r requirements.txt\n","%pip install onnxruntime==1.14.0\n","\n","clear_output()\n","print('Downloading OpenVPI NSF-HiFiGAN')\n","!wget https://github.com/openvpi/vocoders/releases/download/nsf-hifigan-44.1k-hop512-128bin-2024.02/nsf_hifigan_44.1k_hop512_128bin_2024.02.zip\n","!unzip nsf_hifigan_44.1k_hop512_128bin_2024.02.zip -d checkpoints\n","!rm nsf_hifigan_44.1k_hop512_128bin_2024.02.zip\n","\n","clear_output()\n","print('Downloading OpenVPI RMVPE')\n","!wget https://github.com/yxlllc/RMVPE/releases/download/230917/rmvpe.zip\n","!unzip rmvpe.zip -d checkpoints\n","!rm rmvpe.zip\n","\n","clear_output()\n","print('Downloading OpenVPI Harmonic Separator')\n","!wget https://github.com/yxlllc/vocal-remover/releases/download/hnsep_240512/hnsep_240512.zip\n","!unzip hnsep_240512.zip -d checkpoints\n","!rm hnsep_240512.zip\n","\n","clear_output()\n","print('Done')"],"metadata":{"id":"vzj6-UbFcFt9","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Mount Google Drive\n","\n","#@markdown This still makes things easy so just do it.\n","#@markdown ## Note: Because of weird path processing reasons, the Google Drive folder will be mounted in /content/DiffSinger/drive instead of the usual /content/drive\n","\n","from google.colab import drive\n","drive.flush_and_unmount()\n","!rm -rf /content/DiffSinger/drive\n","drive.mount('/content/DiffSinger/drive')\n","print('Done!')"],"metadata":{"cellView":"form","id":"CgafmsJrbRM9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Raw Data Preparation"],"metadata":{"id":"zHgeNsJtcAfl"}},{"cell_type":"code","source":["#@title Decompress dataset\n","\n","#@markdown Make sure it's an actual DiffSinger one. I am not adding automated DB making here sowwy... You need to refine that as much as you can\n","\n","#@markdown Only accepting `.7z` or whatever `p7zip` reads sowwy part two...\n","\n","#@markdown Make sure each speaker is in a folder... basically\n","\n","#@markdown ```\n","#@markdown Archive\n","#@markdown +---speaker1\n","#@markdown |   | transcriptions.csv\n","#@markdown |   \\---wavs\n","#@markdown |       <wav-files/>\n","#@markdown +---speaker2\n","#@markdown |   | transcriptions.csv\n","#@markdown |   \\---wavs\n","#@markdown |       <wav-files/>\n","#@markdown ```\n","\n","#@markdown it'll just assume that it's multispeaker when there's multiple folders of course.\n","\n","#@markdown oh also if you have a dictionary just upload it directly to `DiffSinger/dictionaries` thx.\n","\n","import glob\n","\n","dataset_loc = '/content/DiffSinger/drive/MyDrive/dataset.7z' #@param {type: \"string\"}\n","\n","!7za x \"$dataset_loc\" -o\"data\"\n","\n","folders = glob.glob('data/*')\n","folders.sort()"],"metadata":{"id":"q4E4ziyckLda","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Decompress binaries (Skip if you haven't done binarization)\n","\n","#@markdown This is to skip binarization hopefully.\n","\n","#@markdown If you made binaries outside of this notebook, make sure they're in folders like `acoustic_bin` and `variance_bin` because that's how I formatted them.\n","\n","binary_loc = '/content/DiffSinger/drive/MyDrive/binaries.7z' #@param {type: \"string\"}\n","\n","!7za x \"$binary_loc\" -o\"data\""],"metadata":{"cellView":"form","id":"2vslkB4zdGeQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Data Processing Settings\n","\n","You might still need to run these even if you have binaries."],"metadata":{"id":"ZSgKEkdgokO3"}},{"cell_type":"code","source":["#@title General Settings\n","\n","#@markdown ### Dataset Stuff\n","#@markdown `exp_name` is just for the model folder names now. Speaker names are taken from the folders in the archive.\n","exp_name = 'model' #@param {type: \"string\"}\n","num_test_samples = 3 #@param {type: \"integer\"}\n","dictionary = 'dictionary.txt' #@param {type: \"string\"}\n","\n","#@markdown ---\n","\n","#@markdown ### Variance stuff\n","\n","#@markdown These options need to be present on both models so they're right here.\n","\n","use_energy = False #@param {type: \"boolean\"}\n","use_breathiness = False #@param {type: \"boolean\"}\n","use_tension = False #@param {type: \"boolean\"}\n","use_voicing = False #@param {type: \"boolean\"}\n","\n","import yaml\n","import os\n","import copy\n","\n","def load_yaml(location):\n","    res = None\n","    with open(location) as f:\n","        res = yaml.safe_load(f)\n","    return res\n","\n","def write_yaml(data, location):\n","    with open(location, 'w', encoding='utf8') as f:\n","        yaml.dump(data, f, default_flow_style=False, allow_unicode=True, sort_keys=False)\n","\n","def represent_none(self, _):\n","    return self.represent_scalar('tag:yaml.org,2002:null', '')\n","\n","def get_test_prefixes(folder, n=5, id=None):\n","    wavs = glob.glob(os.path.join(folder, 'wavs/*.wav'))\n","    wavs.sort()\n","    wavs = wavs[-n:]\n","    for i in range(len(wavs)):\n","        _, f = os.path.split(wavs[i])\n","        fn, _ = os.path.splitext(f)\n","        wavs[i] = fn if id is None else f'{id}:{fn}'\n","\n","    return wavs\n","\n","base = load_yaml('configs/base.yaml')\n","\n","#@markdown ---\n","\n","#@markdown ### Batching stuff (for processing)\n","\n","#@markdown This is the best Colab can do. They are not known for having the best CPUs...\n","num_workers = 2 #@param {type: \"slider\", min: 0, max: 2, step: 1}\n","\n","base['ds_workers'] = num_workers\n","\n","#@markdown ---\n","\n","#@markdown ### Pitch Estimator stuff\n","\n","#@markdown `parselmouth` is fast, `rmvpe` is fast but may have range issues, `harvest` is slow but very accurate\n","\n","pitch_estimator = 'harvest' #@param [\"parselmouth\", \"rmvpe\", \"harvest\"]\n","\n","#@markdown Change this if you're using `harvest`. `rmvpe` doesn't read this.\n","\n","f0_min = 65 #@param {type: \"number\"}\n","f0_max = 1100 #@param {type: \"number\"}\n","\n","base['pe'] = pitch_estimator\n","base['f0_min'] = f0_min\n","base['f0_max'] = f0_max\n","\n","if pitch_estimator == 'rmvpe':\n","    base['pe_ckpt'] = 'checkpoints/rmvpe/model.pt'\n","\n","#@markdown ---\n","\n","#@markdown ### Harmonic Separator stuff\n","\n","#@markdown Harmonic separation is done for tension and voicing parameters. You can ignore this freely if you don't plan to have tension/voicing.\n","\n","#@markdown `vr` uses the new AI-based harmonic separator, `world` uses the old WORLD based harmonic separator.\n","\n","hnsep = 'vr' #@param ['vr', 'world']\n","\n","base['hnsep'] = hnsep\n","\n","#@markdown ---\n","\n","#@markdown ### Training Precision stuff\n","\n","#@markdown T4s and V100s don't support `bf16-mixed`\n","\n","precision = '16-mixed' #@param [\"32-true\", \"64-true\", \"16-mixed\", \"bf16-mixed\"]\n","\n","base['pl_trainer_precision'] = precision\n","\n","write_yaml(base, 'configs/base.yaml')\n","\n","num_spk = len(folders)\n","use_spk_id = num_spk > 1\n","test_prefixes = []\n","speakers = copy.deepcopy(folders)\n","for i in range(len(speakers)):\n","    _, s = os.path.split(speakers[i])\n","    speakers[i] = s\n","\n","if not use_spk_id:\n","    folders = folders[0]\n","    test_prefixes = get_test_prefixes(folders, n=num_test_samples)\n","else:\n","    for i in range(len(folders)):\n","        folder = folders[i]\n","        test_prefixes.extend(get_test_prefixes(folder, n=num_test_samples, id=i))\n"],"metadata":{"id":"E4aCTp0ms75q","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Acoustic Settings\n","\n","#@markdown I won't be putting much (a lie) I think you should just directly edit the YAML (still true despite that).\n","\n","yaml.add_representer(type(None), represent_none)\n","\n","acoustic = load_yaml('configs/acoustic.yaml')\n","\n","acoustic['raw_data_dir'] = folders\n","acoustic['binary_data_dir'] = 'data/acoustic_bin'\n","\n","acoustic['speakers'] = speakers\n","acoustic['test_prefixes'] = test_prefixes\n","acoustic['use_spk_id'] = use_spk_id\n","acoustic['num_spk'] = num_spk\n","acoustic['binarization_args']['num_workers'] = num_workers\n","acoustic['dictionary'] = 'dictionaries/' + dictionary\n","acoustic['use_energy_embed'] = use_energy\n","acoustic['use_breathiness_embed'] = use_breathiness\n","acoustic['use_tension_embed'] = use_tension\n","acoustic['use_voicing_embed'] = use_voicing\n","\n","#@markdown ---\n","\n","#@markdown ### Augmentation Stuff\n","#@markdown I honestly still don't fully know what this is for (other than data augmentation and the silly embeds it can give you).\n","\n","#@markdown The scale is for what percentage of the dataset is used for augmentation, so the default is 100% aka the whole dataset.\n","#@markdown\n","\n","#@markdown #### **Pitch Shifting Augmentation**\n","#@markdown Adds more pitch or smn. The default scale here is for `random`, the default scale for `fixed` is `0.75`. `random` enables you to have a gender parameter working. `fixed` apparently makes speakers so it might not be ideal for multispeaker.\n","pitch_augmentation = 'none' #@param [\"none\", \"fixed\", \"random\"]\n","shift_scale = 1.0 #@param {type: \"slider\", min: 0, max: 1, step: 0.01}\n","\n","#@markdown\n","\n","#@markdown #### **Time Stretching Augmentation**\n","#@markdown It... time stretches or smn man idk. Idek what a continuous velocity parameter would be like.\n","time_stretching = False #@param {type: \"boolean\"}\n","stretch_scale = 1.0 #@param {type: \"slider\", min: 0, max: 1, step: 0.01}\n","\n","if pitch_augmentation == 'fixed':\n","    acoustic['augmentation_args']['fixed_pitch_shifting']['enabled'] = True\n","    acoustic['augmentation_args']['random_pitch_shifting']['enabled'] = False\n","    acoustic['augmentation_args']['fixed_pitch_shifting']['scale'] = shift_scale\n","    acoustic['use_key_shift_embed'] = False\n","elif pitch_augmentation == 'random':\n","    acoustic['augmentation_args']['fixed_pitch_shifting']['enabled'] = False\n","    acoustic['augmentation_args']['random_pitch_shifting']['enabled'] = True\n","    acoustic['augmentation_args']['random_pitch_shifting']['scale'] = shift_scale\n","    acoustic['use_key_shift_embed'] = True\n","elif pitch_augmentation == 'none':\n","    acoustic['augmentation_args']['fixed_pitch_shifting']['enabled'] = False\n","    acoustic['augmentation_args']['random_pitch_shifting']['enabled'] = False\n","    acoustic['use_key_shift_embed'] = False\n","\n","acoustic['augmentation_args']['random_time_stretching']['enabled'] = time_stretching\n","acoustic['augmentation_args']['random_time_stretching']['scale'] = stretch_scale\n","acoustic['use_speed_embed'] = time_stretching\n","\n","#@markdown ---\n","\n","#@markdown ### Diffusion Stuff\n","#@markdown Diffusion Type is how the diffusion process is done really. Switching it to `reflow` by default instead of `ddpm`\n","\n","acoustic_diff_type = 'reflow' #@param [\"ddpm\", \"reflow\"]\n","acoustic['diffusion_type'] = acoustic_diff_type\n","\n","#@markdown Shallow diffusion is basically a hybrid solution of rendering acoustic. It's faster and better quality (says in the paper).\n","\n","use_shallow_diffusion = True #@param {type : \"boolean\"}\n","acoustic['use_shallow_diffusion'] = use_shallow_diffusion\n","\n","#@markdown `k_step` is the number of diffusion steps that the diffusion side will take. 200 is enough but 400 is kept cuz it's the default\n","\n","#@markdown `reflow` does not use `k_step` but I'll be converting it to its equivalent `t_start`, which is why this is a slider now.\n","k_step = 400 #@param {type: \"slider\", min: 0, max: 1000, step: 1}\n","k_step = int(k_step)\n","acoustic['K_step'] = k_step\n","acoustic['K_step_infer'] = k_step\n","acoustic['T_start'] = k_step / 1000\n","acoustic['T_start_infer'] = k_step / 1000\n","\n","#@markdown ---\n","\n","#@markdown ### Batching Stuff (for training)\n","\n","max_batch_size = 12 #@param {type: \"integer\"}\n","\n","#@markdown This simulates a larger batch size while having lower memory cost, but makes training \"slower\" since not all batches makes a training step.\n","\n","#@markdown You can put 4 to simulate the original 48 max batch size...\n","accumulate_grad_batches = 1 #@param {type: \"integer\"}\n","\n","acoustic['max_batch_size'] = max_batch_size\n","acoustic['accumulate_grad_batches'] = accumulate_grad_batches\n","\n","#@markdown ---\n","\n","#@markdown ### Learning Rate Stuff\n","\n","#@markdown If your batch size is lower, make your `step_size` higher to accomodate for the slower learning time. Maybe `gamma` too.\n","#@markdown `reflow` is faster at learning than `ddpm`\n","\n","learning_rate = 0.0004 #@param {type: \"number\"}\n","step_size = 50000 #@param {type: \"integer\"}\n","gamma = 0.5 #@param {type: \"slider\", min: 0, max: 1, step: 0.01}\n","\n","acoustic['optimizer_args']['lr'] = learning_rate\n","acoustic['lr_scheduler_args']['step_size'] = step_size\n","acoustic['lr_scheduler_args']['gamma'] = gamma\n","\n","#@markdown ---\n","\n","#@markdown ### Loss Stuff\n","\n","#@markdown `reflow` seems to be better with L1 loss.\n","\n","acoustic_loss_type = 'l2' #@param [\"l1\", \"l2\"]\n","acoustic['main_loss_type'] = acoustic_loss_type\n","\n","write_yaml(acoustic, 'configs/acoustic.yaml')"],"metadata":{"id":"ZXoVIWLXoaxW","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Variance Settings\n","\n","#@markdown Same deal as acoustic\n","\n","#@markdown ---\n","\n","variance = load_yaml('configs/variance.yaml')\n","\n","variance['raw_data_dir'] = folders\n","variance['binary_data_dir'] = 'data/variance_bin'\n","\n","variance['speakers'] = speakers\n","variance['test_prefixes'] = test_prefixes\n","variance['use_spk_id'] = use_spk_id\n","variance['num_spk'] = num_spk\n","variance['dictionary'] = 'dictionaries/' + dictionary\n","variance['binarization_args']['num_workers'] = num_workers\n","\n","#@markdown **Tip:** Duration trains better with pitch/energy/breathiness, but pitch/energy/breathiness trains better without duration.\n","\n","predict_duration = True #@param {type: \"boolean\"}\n","\n","#@markdown ---\n","\n","#@markdown ### Diffusion stuff\n","\n","#@markdown Same deal with acoustic.\n","\n","variance_diff_type = 'reflow' #@param [\"ddpm\", \"reflow\"]\n","variance['diffusion_type'] = variance_diff_type\n","\n","#@markdown ---\n","\n","#@markdown ### Pitch Generation stuff\n","\n","#@markdown Pitch prediction is kinda okay if you train it with L1 loss.\n","\n","predict_pitch = False #@param {type: \"boolean\"}\n","\n","#@markdown These are new things for pitch modelling.\n","#@markdown If I understand correctly, melody encoder is mostly really just distinguishing rests from notes,\n","#@markdown and glide embeds are a new thing that even SlurCutter doesn't even support adding it yet.\n","\n","#@markdown Only melody encoder is supported now I think? Not sure with glide embeds.\n","\n","use_melody_encoder = False #@param {type: \"boolean\"}\n","use_glide_embed = False #@param {type: \"boolean\"}\n","\n","variance['predict_dur'] = predict_duration\n","variance['predict_pitch'] = predict_pitch\n","variance['predict_energy'] = use_energy\n","variance['predict_breathiness'] = use_breathiness\n","variance['predict_tension'] = use_tension\n","variance['predict_voicing'] = use_voicing\n","variance['use_melody_encoder'] = use_melody_encoder\n","variance['use_glide_embed'] = use_glide_embed\n","\n","#@markdown ---\n","\n","#@markdown ### Batching Stuff (for training)\n","\n","#@markdown Lower this if you're training pitch/energy/breathiness. Something like 16 or 20.\n","max_batch_size = 48 #@param {type: \"integer\"}\n","\n","accumulate_grad_batches = 1 #@param {type: \"integer\"}\n","\n","variance['max_batch_size'] = max_batch_size\n","variance['accumulate_grad_batches'] = accumulate_grad_batches\n","\n","#@markdown ---\n","\n","#@markdown ### Learning Rate Stuff\n","\n","#@markdown You can leave this alone but **I highly recommend putting higher learning rates and higher step sizes when training with pitch/energy/breathiness.**\n","#@markdown Something like `lr = 0.001, step_size = 50000` works. Diffusion just has a rough start.\n","\n","#@markdown If you also increase step size you should decrease gamma too. Maybe to like... `0.6`.\n","\n","learning_rate = 0.0006 #@param {type: \"number\"}\n","step_size = 12000 #@param {type: \"integer\"}\n","gamma = 0.75 #@param {type: \"slider\", min: 0, max: 1, step: 0.01}\n","\n","variance['optimizer_args']['lr'] = learning_rate\n","variance['lr_scheduler_args']['step_size'] = step_size\n","variance['lr_scheduler_args']['gamma'] = gamma\n","\n","#@markdown ---\n","\n","#@markdown ### Loss Stuff\n","\n","#@markdown You can leave this alone. MSE = L2 btw.\n","\n","duration_loss_type = 'mse' #@param [\"mse\", \"huber\"]\n","#@markdown I think this is only used when using Pitch Diffusion and Energy/Breathiness prediction. It likes L1 a lot.\n","variance_loss_type = 'l2' #@param [\"l1\", \"l2\"]\n","\n","variance['main_loss_type'] = variance_loss_type\n","variance['dur_prediction_args']['loss_type'] = duration_loss_type\n","\n","write_yaml(variance, 'configs/variance.yaml')"],"metadata":{"id":"bH9FwFxLupHX","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Actual Data Processing"],"metadata":{"id":"qE2w5LBbwu7b"}},{"cell_type":"code","source":["#@title Binarize Variance\n","!python scripts/binarize.py --config configs/variance.yaml"],"metadata":{"id":"p-S6GPANwzGI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Binarize Acoustic\n","!python scripts/binarize.py --config configs/acoustic.yaml"],"metadata":{"id":"hUnesxHTxCnQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Compress binaries\n","\n","#@markdown This saves a `binaries.7z` in your root folder in Google Drive a.k.a it'll just be outside everything\n","%cd data\n","!7za a binaries.7z acoustic_bin variance_bin\n","!mv binaries.7z /content/DiffSinger/drive/MyDrive/\n","%cd .."],"metadata":{"cellView":"form","id":"tju3DnO4chK8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training\n","\n","## Note: I highly recommend saving to Drive. You don't really wanna lose the models."],"metadata":{"id":"DvvYOT0VzqJG"}},{"cell_type":"code","source":["#@title Launch Tensorboard\n","\n","#@markdown I hope Tensorboard doesn't have the same issues as it did back then but just in case... Do this if it doesn't load.\n","\n","#@markdown ## Chromium\n","#@markdown - Enable third party cookies.\n","\n","#@markdown ## Firefox\n","#@markdown - Disable Enhanced Tracking for Google Colab.\n","\n","#@markdown **TIP:** You can set a reload interval if you click the settings at the top to get updates every 30 seconds or so\n","\n","checkpoints_in_drive = True #@param {type: \"boolean\"}\n","\n","%load_ext tensorboard\n","if checkpoints_in_drive:\n","    %tensorboard --logdir /content/DiffSinger/drive/MyDrive/DiffSinger_Checkpoints\n","else:\n","    %tensorboard --logdir /content/DiffSinger/checkpoints"],"metadata":{"cellView":"form","id":"pSX78d9Lshpw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Train Variance\n","import os\n","import yaml\n","\n","DRIVE_FOLDER = '/content/DiffSinger/drive/MyDrive/DiffSinger_Checkpoints'\n","save_to_drive = True #@param {type: \"boolean\"}\n","\n","def load_yaml(location):\n","    res = None\n","    with open(location) as f:\n","        res = yaml.safe_load(f)\n","    return res\n","\n","def write_yaml(data, location):\n","    with open(location, 'w', encoding='utf8') as f:\n","        yaml.dump(data, f, default_flow_style=False, allow_unicode=True, sort_keys=False)\n","\n","def represent_none(self, _):\n","    return self.represent_scalar('tag:yaml.org,2002:null', '')\n","\n","def resolve_config(config):\n","    res = load_yaml(config)\n","    if 'base_config' in res.keys():\n","        for c in res['base_config']:\n","            temp = resolve_config(c)\n","            for k, v in temp.items():\n","                if k not in res.keys():\n","                    res[k] = v\n","        del res['base_config']\n","    return res\n","\n","if save_to_drive:\n","    if not os.path.exists(DRIVE_FOLDER):\n","        os.makedirs(DRIVE_FOLDER)\n","\n","    final_config_path = os.path.join(DRIVE_FOLDER, f'{exp_name}_variance/config.yaml')\n","    if not os.path.exists(final_config_path):\n","        dir = os.path.dirname(final_config_path)\n","        os.makedirs(dir)\n","        final_config = resolve_config('configs/variance.yaml')\n","        write_yaml(final_config, final_config_path)\n","    !python scripts/train.py --config configs/variance.yaml --reset --hparams work_dir={DRIVE_FOLDER}/{exp_name}_variance\n","else:\n","    !python scripts/train.py --config configs/variance.yaml --exp_name {exp_name}_variance --reset"],"metadata":{"id":"3-wMa7_Rz3Ok","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Train Acoustic\n","import os\n","\n","DRIVE_FOLDER = '/content/DiffSinger/drive/MyDrive/DiffSinger_Checkpoints'\n","save_to_drive = True #@param {type: \"boolean\"}\n","\n","def load_yaml(location):\n","    res = None\n","    with open(location) as f:\n","        res = yaml.safe_load(f)\n","    return res\n","\n","def write_yaml(data, location):\n","    with open(location, 'w', encoding='utf8') as f:\n","        yaml.dump(data, f, default_flow_style=False, allow_unicode=True, sort_keys=False)\n","\n","def represent_none(self, _):\n","    return self.represent_scalar('tag:yaml.org,2002:null', '')\n","\n","def resolve_config(config):\n","    res = load_yaml(config)\n","    if 'base_config' in res.keys():\n","        for c in res['base_config']:\n","            temp = resolve_config(c)\n","            for k, v in temp.items():\n","                if k not in res.keys():\n","                    res[k] = v\n","        del res['base_config']\n","    return res\n","\n","if save_to_drive:\n","    if not os.path.exists(DRIVE_FOLDER):\n","        os.makedirs(DRIVE_FOLDER)\n","\n","    final_config_path = os.path.join(DRIVE_FOLDER, f'{exp_name}_acoustic/config.yaml')\n","    if not os.path.exists(final_config_path):\n","        dir = os.path.dirname(final_config_path)\n","        os.makedirs(dir)\n","        final_config = resolve_config('configs/acoustic.yaml')\n","        write_yaml(final_config, final_config_path)\n","    !python scripts/train.py --config configs/acoustic.yaml --reset --hparams work_dir={DRIVE_FOLDER}/{exp_name}_acoustic\n","else:\n","    !python scripts/train.py --config configs/acoustic.yaml --exp_name {exp_name}_acoustic --reset"],"metadata":{"id":"t6s0rvjJ0dNJ","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Export to ONNX\n","\n","You can format your stuff for OU urself sowwy.. just look at [this](https://github.com/xunmengshe/OpenUtau/wiki/Voicebank-Development) as a guide. also the models need to be in `DiffSinger/checkpoints` if you're exporting.\n","\n","## NOTE: If you know how to run DiffSinger locally, please just use local DiffSinger to export. It works ONLY in PyTorch 1.13."],"metadata":{"id":"Fpwq6xRK0hCJ"}},{"cell_type":"code","source":["#@title Install PyTorch 1.13\n","\n","#@markdown If you still wanna export ONNX here in Colab...\n","\n","%pip install -U torch==1.13.0+cu117 torchvision==0.14.0+cu117 torchaudio==0.13.0 --extra-index-url https://download.pytorch.org/whl/cu117"],"metadata":{"cellView":"form","id":"DYwHAX4yxNci"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Move Drive checkpoints\n","!cp -dpR /content/DiffSinger/drive/MyDrive/DiffSinger_Checkpoints /content/DiffSinger/checkpoints"],"metadata":{"cellView":"form","id":"nh5Q5jhBBYsD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Exporter\n","model_type = 'acoustic' #@param [\"acoustic\", \"variance\"]\n","speaker_name = 'speaker1' #@param {type: \"string\"}\n","!python scripts/export.py {model_type} --exp {speaker_name}_{model_type}"],"metadata":{"id":"D4dE71tt0sm-"},"execution_count":null,"outputs":[]}]}